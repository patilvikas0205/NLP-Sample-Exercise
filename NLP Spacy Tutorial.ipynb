{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Spacy Module \n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step to install spacy\n",
    "# pip install spacy\n",
    "# python -m spacy download en_core_web_sm\n",
    "# python -m spacy download en_core_web_lg\n",
    "# python -m spacy download en_core_web_md\n",
    "# python -m spacy download en\n",
    "# English multi-task CNN trained on OntoNotes, \n",
    "# with GloVe vectors trained on Common Crawl. Assigns word vectors, POS tags, dependency parses and named entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import \n",
    "import en_core_web_sm\n",
    "nlp = en_core_web_sm.load()\n",
    "#nlp= spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = ('''Covaxin, which has been jointly developed by Bharat Biotech and the ICMR, had received the \n",
    "            restricted emergency use authorisation by the apex drug regulator in India along with Oxford\n",
    "            University-AstraZeneca’s Covishield, being manufactured in India by the Serum Institute of India. \n",
    "            The approval of Covaxin, which is still under phase 3 trial in the country, had triggered a torrent of criticism, \n",
    "            but a key reason cited by the drug regulator had said that the REU was being granted in the wake of the \n",
    "            mutation in Covid-19 that makes it more transmissible.''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc=nlp(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14626626061804382878"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc.lang"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Covaxin, which has been jointly developed by Bharat Biotech and the ICMR, had received the \n",
       "            restricted emergency use authorisation by the apex drug regulator in India along with Oxford\n",
       "            University-AstraZeneca’s Covishield, being manufactured in India by the Serum Institute of India. \n",
       "            The approval of Covaxin, which is still under phase 3 trial in the country, had triggered a torrent of criticism, \n",
       "            but a key reason cited by the drug regulator had said that the REU was being granted in the wake of the \n",
       "            mutation in Covid-19 that makes it more transmissible."
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Noun phrases:',\n",
       " ['Covaxin',\n",
       "  'Bharat Biotech',\n",
       "  'the ICMR',\n",
       "  'the \\n            restricted emergency use authorisation',\n",
       "  'the apex drug regulator',\n",
       "  'India',\n",
       "  'Oxford\\n            University-AstraZeneca',\n",
       "  'Covishield',\n",
       "  'India',\n",
       "  'the Serum Institute',\n",
       "  'India',\n",
       "  'The approval',\n",
       "  'Covaxin',\n",
       "  'phase',\n",
       "  '3 trial',\n",
       "  'the country',\n",
       "  'a torrent',\n",
       "  'criticism',\n",
       "  'a key reason',\n",
       "  'the drug regulator',\n",
       "  'the REU',\n",
       "  'the wake',\n",
       "  'the \\n            mutation',\n",
       "  'it'])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"Noun phrases:\",[token.text for token in doc.noun_chunks]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Noun phrases:',\n",
       " ['emergency',\n",
       "  'use',\n",
       "  'authorisation',\n",
       "  'drug',\n",
       "  'regulator',\n",
       "  'approval',\n",
       "  'phase',\n",
       "  'trial',\n",
       "  'country',\n",
       "  'torrent',\n",
       "  'criticism',\n",
       "  'reason',\n",
       "  'drug',\n",
       "  'regulator',\n",
       "  'wake',\n",
       "  'mutation'])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#in this case pronoun not consider only consider noun\n",
    "\"Noun phrases:\",[token.text for token in doc if token.pos_ == \"NOUN\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('verb phrases:',\n",
       " [developed,\n",
       "  received,\n",
       "  restricted,\n",
       "  manufactured,\n",
       "  triggered,\n",
       "  cited,\n",
       "  said,\n",
       "  granted,\n",
       "  makes])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#only extract verb without lemmatizing\n",
    "\"verb phrases:\",[token for token in doc if token.pos_ == \"VERB\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('verb phrases:',\n",
       " ['develop',\n",
       "  'receive',\n",
       "  'restrict',\n",
       "  'manufacture',\n",
       "  'trigger',\n",
       "  'cite',\n",
       "  'say',\n",
       "  'grant',\n",
       "  'make'])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Above its hows onlt verb as its is now we apply lemmatize\n",
    "# token.lemma_\n",
    "\"verb phrases:\",[token.lemma_ for token in doc if token.pos_ == \"VERB\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Covaxin ORG\n",
      "Bharat Biotech PERSON\n",
      "India GPE\n",
      "Oxford\n",
      "            University-AstraZeneca’s ORG\n",
      "India GPE\n",
      "the Serum Institute of India ORG\n",
      "Covaxin GPE\n",
      "3 CARDINAL\n"
     ]
    }
   ],
   "source": [
    "for entity in doc.ents:\n",
    "    print(entity.text, entity.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Covaxin ---> PROPN\n",
      ", ---> PUNCT\n",
      "which ---> DET\n",
      "has ---> AUX\n",
      "been ---> AUX\n",
      "jointly ---> ADV\n",
      "developed ---> VERB\n",
      "by ---> ADP\n",
      "Bharat ---> PROPN\n",
      "Biotech ---> PROPN\n",
      "and ---> CCONJ\n",
      "the ---> DET\n",
      "ICMR ---> PROPN\n",
      ", ---> PUNCT\n",
      "had ---> AUX\n",
      "received ---> VERB\n",
      "the ---> DET\n",
      "\n",
      "             ---> SPACE\n",
      "restricted ---> VERB\n",
      "emergency ---> NOUN\n",
      "use ---> NOUN\n",
      "authorisation ---> NOUN\n",
      "by ---> ADP\n",
      "the ---> DET\n",
      "apex ---> PROPN\n",
      "drug ---> NOUN\n",
      "regulator ---> NOUN\n",
      "in ---> ADP\n",
      "India ---> PROPN\n",
      "along ---> ADP\n",
      "with ---> ADP\n",
      "Oxford ---> PROPN\n",
      "\n",
      "             ---> SPACE\n",
      "University ---> PROPN\n",
      "- ---> PUNCT\n",
      "AstraZeneca ---> PROPN\n",
      "’s ---> PROPN\n",
      "Covishield ---> PROPN\n",
      ", ---> PUNCT\n",
      "being ---> AUX\n",
      "manufactured ---> VERB\n",
      "in ---> ADP\n",
      "India ---> PROPN\n",
      "by ---> ADP\n",
      "the ---> DET\n",
      "Serum ---> PROPN\n",
      "Institute ---> PROPN\n",
      "of ---> ADP\n",
      "India ---> PROPN\n",
      ". ---> PUNCT\n",
      "\n",
      "             ---> SPACE\n",
      "The ---> DET\n",
      "approval ---> NOUN\n",
      "of ---> ADP\n",
      "Covaxin ---> PROPN\n",
      ", ---> PUNCT\n",
      "which ---> DET\n",
      "is ---> AUX\n",
      "still ---> ADV\n",
      "under ---> ADP\n",
      "phase ---> NOUN\n",
      "3 ---> NUM\n",
      "trial ---> NOUN\n",
      "in ---> ADP\n",
      "the ---> DET\n",
      "country ---> NOUN\n",
      ", ---> PUNCT\n",
      "had ---> AUX\n",
      "triggered ---> VERB\n",
      "a ---> DET\n",
      "torrent ---> NOUN\n",
      "of ---> ADP\n",
      "criticism ---> NOUN\n",
      ", ---> PUNCT\n",
      "\n",
      "             ---> SPACE\n",
      "but ---> CCONJ\n",
      "a ---> DET\n",
      "key ---> ADJ\n",
      "reason ---> NOUN\n",
      "cited ---> VERB\n",
      "by ---> ADP\n",
      "the ---> DET\n",
      "drug ---> NOUN\n",
      "regulator ---> NOUN\n",
      "had ---> AUX\n",
      "said ---> VERB\n",
      "that ---> SCONJ\n",
      "the ---> DET\n",
      "REU ---> PROPN\n",
      "was ---> AUX\n",
      "being ---> AUX\n",
      "granted ---> VERB\n",
      "in ---> ADP\n",
      "the ---> DET\n",
      "wake ---> NOUN\n",
      "of ---> ADP\n",
      "the ---> DET\n",
      "\n",
      "             ---> SPACE\n",
      "mutation ---> NOUN\n",
      "in ---> ADP\n",
      "Covid-19 ---> PROPN\n",
      "that ---> DET\n",
      "makes ---> VERB\n",
      "it ---> PRON\n",
      "more ---> ADV\n",
      "transmissible ---> ADJ\n",
      ". ---> PUNCT\n"
     ]
    }
   ],
   "source": [
    "#part of speech taging using spacy\n",
    "'''In English grammar, the parts of speech tell us what is the function of a word and how it is used in a sentence. \n",
    "Some of the common parts of speech in English are Noun, Pronoun, Adjective, Verb, Adverb, etc.POS tagging is the task \n",
    "of automatically assigning POS tags to all the words of a sentence. It is helpful in various downstream tasks in NLP, \n",
    "such as feature engineering, language understanding, and information extraction.\n",
    "Performing POS tagging, in spaCy, is a cakewalk:'''\n",
    "\n",
    "for token in doc:\n",
    "    print(token.text, \"--->\", token.pos_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#doc=nlp(text)\n",
    "# for token in doc.noun_chunks---> print token.text --> extract noun ,  prnoun all\n",
    "# for token in doc if token.pos_ == \"NOUN\" ---> print token.text ---> only pronon\n",
    "# for token in doc if token.pos_ == \"VERB\" ----> print token.text ---> only verb without lemmeatizer\n",
    "# for token in doc if token.pos_ == \"VERB\" ---> print token.lemma_ ---> lammtize eg reading ---> read \n",
    "# for entity in doc.ents ---> print entity.text. entity.label_  eg vikas - person,  google - org\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import spacy\n",
    "# import en_core_web_sm\n",
    "# nlp = en_core_web_sm.load()\n",
    "# nlp = spacy.load('en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'adposition'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spacy.explain('ADP')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "text=\"Google News is a news aggregator service developed by Google.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc=nlp(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Google ---> compound\n",
      "News ---> nsubj\n",
      "is ---> ROOT\n",
      "a ---> det\n",
      "news ---> compound\n",
      "aggregator ---> compound\n",
      "service ---> attr\n",
      "developed ---> acl\n",
      "by ---> agent\n",
      "Google ---> pobj\n",
      ". ---> punct\n"
     ]
    }
   ],
   "source": [
    "# Dependancy parsing\n",
    "for token in doc:\n",
    "    print(token.text, \"--->\", token.dep_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indians ----> NORP\n",
      "$71 billion ----> MONEY\n",
      "2018 ----> DATE\n"
     ]
    }
   ],
   "source": [
    "# Named Entity Recognition using spaCy\n",
    "doc=nlp(\"Indians spent over $71 billion on clothes in 2018\")\n",
    "\n",
    "for token in doc.ents:\n",
    "    print(token.text,\"---->\",token.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Nationalities or religious or political groups'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "spacy.explain(\"NORP\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indians ----> PROPN\n",
      "spent ----> VERB\n",
      "over ----> ADP\n",
      "$ ----> SYM\n",
      "71 ----> NUM\n",
      "billion ----> NUM\n",
      "on ----> ADP\n",
      "clothes ----> NOUN\n",
      "in ----> ADP\n",
      "2018 ----> NUM\n"
     ]
    }
   ],
   "source": [
    "#pos--> part of speech\n",
    "for token in doc:\n",
    "    print(token.text,\"---->\",token.pos_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indians ----> nsubj\n",
      "spent ----> ROOT\n",
      "over ----> quantmod\n",
      "$ ----> quantmod\n",
      "71 ----> compound\n",
      "billion ----> dobj\n",
      "on ----> prep\n",
      "clothes ----> pobj\n",
      "in ----> prep\n",
      "2018 ----> pobj\n"
     ]
    }
   ],
   "source": [
    "# Dependancy grammer\n",
    "for token in doc:\n",
    "    print(token.text,\"---->\",token.dep_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rule-Based Matching using spaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import en_core_web_sm\n",
    "from spacy.matcher import Matcher\n",
    "\n",
    "nlp=en_core_web_sm.load()\n",
    "\n",
    "matcher = Matcher(nlp.vocab)\n",
    "\n",
    "\n",
    "text=\"Upcoming iPhone X release date leaked\"\n",
    "\n",
    "doc=nlp(text)\n",
    "\n",
    "#define rule\n",
    "pattern=[{'TEXT': 'iPhone'}, {'TEXT':'X'}]\n",
    "\n",
    "# add rule\n",
    "matcher.add('rule1',None, pattern)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(15137773209560627690, 1, 3)]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matches=matcher(doc)\n",
    "matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iPhone X\n"
     ]
    }
   ],
   "source": [
    "for matid , start, end in matches:\n",
    "    matched_span=doc[start:end]\n",
    "    print(matched_span.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total matches found: 3\n",
      "Match found: iOS 7\n",
      "Match found: iOS 11\n",
      "Match found: iOS 10\n"
     ]
    }
   ],
   "source": [
    "# Write one pattern that only matches mentions of the full iOS versions: “iOS 7”, “iOS 11” and “iOS 10”.\n",
    "import spacy\n",
    "from spacy.matcher import Matcher\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "matcher = Matcher(nlp.vocab)\n",
    "\n",
    "doc = nlp(\n",
    "    \"After making the iOS update you won't notice a radical system-wide \"\n",
    "    \"redesign: nothing like the aesthetic upheaval we got with iOS 7. Most of \"\n",
    "    \"iOS 11's furniture remains the same as in iOS 10. But you will discover \"\n",
    "    \"some tweaks once you delve a little deeper.\"\n",
    ")\n",
    "\n",
    "# Write a pattern for full iOS versions (\"iOS 7\", \"iOS 11\", \"iOS 10\")\n",
    "pattern = [{\"TEXT\":\"iOS\"}, {\"IS_DIGIT\": True}]\n",
    "\n",
    "# Add the pattern to the matcher and apply the matcher to the doc\n",
    "matcher.add(\"IOS_VERSION_PATTERN\", None, pattern)\n",
    "matches = matcher(doc)\n",
    "print(\"Total matches found:\", len(matches))\n",
    "\n",
    "# Iterate over the matches and print the span text\n",
    "for match_id, start, end in matches:\n",
    "    print(\"Match found:\", doc[start:end].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total matches found: 2\n",
      "Match found: downloaded Fortnite\n",
      "Match found: downloading Minecraft\n"
     ]
    }
   ],
   "source": [
    "#Write one pattern that only matches forms of “download” (tokens with the lemma “download”), followed by a \n",
    "#token with the part-of-speech tag \"PROPN\" (proper noun).\n",
    "\n",
    "import spacy\n",
    "from spacy.matcher import Matcher\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "matcher = Matcher(nlp.vocab)\n",
    "\n",
    "doc = nlp(\n",
    "    \"i downloaded Fortnite on my laptop and can't open the game at all. Help? \"\n",
    "    \"so when I was downloading Minecraft, I got the Windows version where it \"\n",
    "    \"is the '.zip' folder and I used the default program to unpack it... do \"\n",
    "    \"I also need to download Winzip?\"\n",
    ")\n",
    "\n",
    "# Write a pattern that matches a form of \"download\" plus proper noun\n",
    "pattern = [{\"LEMMA\": \"download\"}, {\"POS\":\"PROPN\"}]\n",
    "\n",
    "# Add the pattern to the matcher and apply the matcher to the doc\n",
    "matcher.add(\"DOWNLOAD_THINGS_PATTERN\", None, pattern)\n",
    "matches = matcher(doc)\n",
    "print(\"Total matches found:\", len(matches))\n",
    "\n",
    "# Iterate over the matches and print the span text\n",
    "for match_id, start, end in matches:\n",
    "    print(\"Match found:\", doc[start:end].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total matches found: 1\n",
      "Match found: optional voice responses\n"
     ]
    }
   ],
   "source": [
    "#Write one pattern that matches adjectives (\"ADJ\") followed by one or two \"NOUN\"s (one noun and one optional noun).\n",
    "import spacy\n",
    "from spacy.matcher import Matcher\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "matcher = Matcher(nlp.vocab)\n",
    "\n",
    "doc = nlp(\n",
    "    \"Features of the app include a beautiful design, smart search, automatic \"\n",
    "    \"labels and optional voice responses.\"\n",
    ")\n",
    "\n",
    "# Write a pattern for adjective plus one or two nouns\n",
    "pattern = [{\"POS\": \"ADJ\"}, {\"POS\": \"NOUN\"}, {\"POS\": \"NOUN\", \"OP\": \"+\"}]\n",
    "\n",
    "# Add the pattern to the matcher and apply the matcher to the doc\n",
    "matcher.add(\"ADJ_NOUN_PATTERN\", None, pattern)\n",
    "matches = matcher(doc)\n",
    "print(\"Total matches found:\", len(matches))\n",
    "\n",
    "# Iterate over the matches and print the span text\n",
    "for match_id, start, end in matches:\n",
    "    print(\"Match found:\", doc[start:end].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2644858412616767388\n",
      "Bowie\n"
     ]
    }
   ],
   "source": [
    "# Data Structure String to Hash\n",
    "\n",
    "from spacy.lang.en import English\n",
    "from spacy.lang.de import German\n",
    "\n",
    "#from.spacy.lang.fr import French \n",
    "\n",
    "# Create an English and German nlp object\n",
    "nlp = English()\n",
    "nlp_de = German()\n",
    "#nlp_fr= French()\n",
    "\n",
    "doc=nlp(\"Bowie\")\n",
    "doc=nlp_de(\"Bowie\")\n",
    "#doc=nlp_fr(\"Bowie\")\n",
    "\n",
    "# Get the ID for the string 'Bowie'\n",
    "bowie_id = nlp.vocab.strings[\"Bowie\"]\n",
    "print(bowie_id)\n",
    "\n",
    "# Look up the ID for \"Bowie\" in the vocab\n",
    "print(nlp_de.vocab.strings[bowie_id])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
